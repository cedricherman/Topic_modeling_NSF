{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw data into a structured format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data is organized by folder named after fiscal years*. Each folder is composed of multiple xml files. Each xml file contains information about one award.\n",
    "There is no detailed description of the award data besides the xml tree composition. Some tags are self explanatory while others needed some investigation by throughout fully reading the NSF website.\n",
    "Here is a non-exhaustive list of tags and its description:\n",
    "\n",
    "1. **AwardTitle:** Title of award\n",
    " \n",
    "2. **AwardEffectiveDate:** Month,Day,Year when funding started\n",
    "\n",
    "3. **AwardExpirationDate:** Month,Day,Year when funding ended\n",
    "\n",
    "4. **AwardAmount:** Amount of money in USD awarded to date\n",
    "\n",
    "5. **AwardInstrument:** Award type (Standard Grant, Continuing Grant,...)\n",
    "\n",
    "6. **Organization:** NSF organization (Directorate and related Division) funding the grant\n",
    "\n",
    "7. **Investigator:** name of supervisor(s) (Principal Investigator, Co-Principal Investigator,...), contact info,...\n",
    "\n",
    "8. **Institution:** name of institution(s) receiving the award, phone number(s), address(es), \n",
    "\n",
    "9. **AwardID:** unique 7 digits identifiers of award\n",
    "\n",
    "*fiscal year Y starts October 1st,Y-1 and ends September 30th,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xml files are unstructured because there could be missing tags or new tags added over the years.\n",
    "From 1960 to 2017, there are about 450,000 awards which means as many files to read!\n",
    "\n",
    "Therefore one solution is to condense all that data into 2 CSV files. One containing \"short\" information (low byte size) and another one containing \"long\" information (Basically just ID and abstract) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract relevant xml tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_tag(input_xml, filename):\n",
    "    \"\"\"\n",
    "        sort out input_soup tags and return list of values\n",
    "        INPUT: input_xml is the content read from a xml file\n",
    "        RETURN two dictionaries, one for short information and one for abstracts\n",
    "    \"\"\"\n",
    "    # dictionaries to record data\n",
    "    shortinfo = {}\n",
    "    abstract = {}\n",
    "    \n",
    "    # make soup and extract tags\n",
    "    input_soup = BeautifulSoup(input_xml, 'lxml-xml')\n",
    "    \n",
    "    # anything fails, that means file structure is corrupted\n",
    "    try:\n",
    "        # award identification (keep it as top level key value)\n",
    "        award_id_string = input_soup.find('AwardID').text\n",
    "        try:\n",
    "            award_id = int(award_id_string)\n",
    "        except:\n",
    "            award_id = award_id_string\n",
    "            warnings.warn('Could NOT convert award id {} to an integer'.format(award_id_string), UserWarning)\n",
    "\n",
    "        # create dictionary for short information\n",
    "        award_elements = {}\n",
    "        \n",
    "        # Title, dates, amount\n",
    "        award_elements['title'] = input_soup.find('AwardTitle').text\n",
    "        award_elements['eff_date'] = input_soup.find('AwardEffectiveDate').text\n",
    "        award_elements['exp_date'] = input_soup.find('AwardExpirationDate').text \n",
    "        amount_string = input_soup.find('AwardAmount').text\n",
    "        # attempt to convert amount to integer\n",
    "        try:\n",
    "            award_elements['amount'] = int(amount_string)\n",
    "        except:\n",
    "            award_elements['amount'] = amount_string\n",
    "            warnings.warn('Could NOT convert awarded amount {} to an integer'.format(amount_string), UserWarning)\n",
    "\n",
    "        # award type\n",
    "        award_elements['award_instr'] = input_soup.find('AwardInstrument').find('Value').text\n",
    "\n",
    "        # organization info\n",
    "        org_tree = input_soup.find('Organization')\n",
    "        org_code_string = org_tree.find('Code').text\n",
    "        try:\n",
    "            award_elements['org_code'] = int(org_code_string)\n",
    "        except:\n",
    "            award_elements['org_code'] = org_code_string\n",
    "            warnings.warn('Could NOT convert org code {} to an integer'.format(org_code_string), UserWarning)\n",
    "\n",
    "        award_elements['org_direct'] = org_tree.find('Directorate').find('LongName').text\n",
    "        award_elements['org_div'] = org_tree.find('Division').find('LongName').text\n",
    "\n",
    "        # nsf officer who approved grant\n",
    "        award_elements['nsf_officer'] = input_soup.find('ProgramOfficer').find('SignBlockName').text\n",
    "\n",
    "        # record all investigator\n",
    "        award_elements['Investigator'] = []\n",
    "        inv_trees = input_soup.Award.find_all('Investigator', recursive=False)\n",
    "        for inv in inv_trees:\n",
    "            this_investigator = {}\n",
    "            this_investigator['FirstName'] =  inv.find('FirstName').text\n",
    "            this_investigator['LastName'] =  inv.find('LastName').text\n",
    "            this_investigator['Role'] = inv.find('RoleCode').text\n",
    "            award_elements['Investigator'].append(this_investigator)\n",
    "\n",
    "        # record all participating institutions\n",
    "        award_elements['Institution'] = []\n",
    "        institution_trees = input_soup.Award.find_all('Institution', recursive=False)\n",
    "        for ins in institution_trees:\n",
    "            this_institution = {}\n",
    "            this_institution['Name'] =  ins.find('Name').text\n",
    "            this_institution['StreetAddress'] =  ins.find('StreetAddress').text\n",
    "            this_institution['City'] = ins.find('CityName').text\n",
    "            this_institution['State'] = ins.find('StateCode').text\n",
    "            this_institution['Country'] = ins.find('CountryName').text\n",
    "            award_elements['Institution'].append(this_institution)\n",
    "\n",
    "        # record program elements (research qualifier)\n",
    "        award_elements['ProgramElement'] = []\n",
    "        progele_trees = input_soup.Award.find_all('ProgramElement', recursive=False)\n",
    "        for pe in progele_trees:\n",
    "            this_progelement = {}\n",
    "            this_progelement['Text'] =  pe.find('Text').text\n",
    "            code_string =  pe.find('Code').text\n",
    "            try:\n",
    "                this_progelement['Code'] = int(code_string)\n",
    "            except:\n",
    "                this_progelement['Code'] = code_string\n",
    "                warnings.warn('Could NOT convert Program element code {} to an integer'.format(code_string), UserWarning)\n",
    "            award_elements['ProgramElement'].append(this_progelement)\n",
    "\n",
    "        # add award id as top level key\n",
    "        shortinfo[award_id] = award_elements\n",
    "        \n",
    "        # take care of abstract\n",
    "        this_abstract = input_soup.find('AbstractNarration').text\n",
    "        # make sure abstract is not empty (tag can exist but text associated with it)\n",
    "        if not not this_abstract:\n",
    "            abstract[award_id] = this_abstract\n",
    "    except:\n",
    "        warnings.warn( \\\n",
    "            'File {} does not comply with xml schema! It will be skipped'.format(os.path.basename(filename)), UserWarning)\n",
    "        \n",
    "    # return both dictionaries\n",
    "    return shortinfo, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_extract(file_list):\n",
    "    \"\"\"\n",
    "        Read files and extract info using Beautiful soup\n",
    "        INPUT: file_list is a list of file names\n",
    "        RETURN two list of dictionaries (short info and abstract)\n",
    "    \"\"\"\n",
    "    # list of dictionaries for short element\n",
    "    awards_short_elements = []\n",
    "    # list of dictionaries for abstract\n",
    "    awards_abstract = []\n",
    "    \n",
    "    # read data in each xml file\n",
    "    for thisfile in file_list:\n",
    "\n",
    "        with open(thisfile, encoding='utf-8') as f:\n",
    "            xml_text = f.read()\n",
    "        \n",
    "        # extract info from xml\n",
    "        award_info, award_text = extract_xml_tag(xml_text, thisfile)\n",
    "        \n",
    "        # populate list of dictionaries unless it is empty\n",
    "        if not not award_info:\n",
    "            awards_short_elements.append(award_info)\n",
    "        if not not award_text:\n",
    "            awards_abstract.append(award_text)\n",
    "        \n",
    "    return awards_short_elements, awards_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare variable to loop over all xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure output csv files do not exist, otherwise delete them\n",
    "short_elements_output = os.path.join(os.pardir,'data', 'interim', 'test_short_element.json')   \n",
    "if os.path.isfile(short_elements_output):\n",
    "    os.remove(short_elements_output)\n",
    "\n",
    "abstract_output = os.path.join(os.pardir,'data', 'interim', 'test_abstract.json')\n",
    "if os.path.isfile(abstract_output):\n",
    "    os.remove(abstract_output)\n",
    "\n",
    "# number of processes (quad cores have 8 CPU, 1 CPU = 1 process at most)\n",
    "NUM_PROCESS = 8\n",
    "\n",
    "# file count and cumulative file count read\n",
    "cumfilecount = 0\n",
    "\n",
    "# number of files to distribute to each task\n",
    "num_file_partition = 200\n",
    "\n",
    "# year range for url, REMINDER: start at 1960\n",
    "years = range(1960,2017+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # get start time of timer for processing time\n",
    "    start_time = time.time()\n",
    "    # create pool\n",
    "    pool = multiprocessing.Pool(processes=NUM_PROCESS, maxtasksperchild=None)\n",
    "\n",
    "    for ny,y in enumerate(years):\n",
    "\n",
    "        # number of files read in current folder\n",
    "        filecount = 0\n",
    "\n",
    "        # list all xml files in current folder\n",
    "        xml_list = glob.glob(os.path.join(os.pardir, 'data', 'raw', str(y), '*.xml'))\n",
    "\n",
    "        # partition list of files\n",
    "        intervals = range(0,len(xml_list), num_file_partition)\n",
    "        xml_partition = [ xml_list[nfile:nfile+num_file_partition] for nfile in intervals ]\n",
    "\n",
    "        # number of task to distribute among cores\n",
    "        num_task = len(xml_partition)\n",
    "\n",
    "        # number of file in last task (most likely different from num_file_partition)\n",
    "        num_lastfile_partition = len(xml_partition[-1])\n",
    "\n",
    "        # feed pool with all files from current year\n",
    "        pool_short, pool_abstract = pool.map(read_extract, xml_partition)\n",
    "\n",
    "        # unpack list of list\n",
    "        short_element = list(chain.from_iterable(pool_short))\n",
    "        abstract = list(chain.from_iterable(pool_abstract))\n",
    "\n",
    "        # write dict to file\n",
    "        with open(short_elements_output, \"a\", encoding='utf-8') as f:\n",
    "            json.dump(short_element, f, ensure_ascii=False)\n",
    "\n",
    "        with open(abstract_output, \"a\", encoding='utf-8') as f:\n",
    "            json.dump(abstract, f, ensure_ascii=False)\n",
    "\n",
    "        # file counters\n",
    "        filecount += (num_task - 1)*num_file_partition + num_lastfile_partition\n",
    "        cumfilecount += filecount\n",
    "\n",
    "        # print progress\n",
    "        print('\\rYear {}, File #{:6d},Total File {:6d}'.format(y, filecount, cumfilecount) ,end='', flush=True)\n",
    "\n",
    "\n",
    "    # close pool\n",
    "    pool.close()\n",
    "    # make sure all processes are fisnished, map() does it too!\n",
    "    pool.join()\n",
    "\n",
    "    # closing print statement\n",
    "    print('\\rYear {}, File #{:6d},Total File {:6d}'.format(y, filecount, cumfilecount), end='\\n', flush=True)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
